{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14ade15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,\\\n",
    "f1_score, precision_recall_fscore_support\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd6727",
   "metadata": {},
   "source": [
    "## 1.\n",
    "Fit the Random Forest classifier to your training sample and transform (i.e. make predictions on the training sample) setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d8c8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = prepare.wrangle_data('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c596dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = train.drop(columns=['sex','embark_town','survived']).columns.to_list()\n",
    "y_cols = 'survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a09635",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[X_cols]\n",
    "y_train = train[y_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d386c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the random forest classifier object\n",
    "# setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10.\n",
    "rf1 = RandomForestClassifier(max_depth=10, min_samples_leaf=1, random_state=123)\n",
    "# fit the rf object\n",
    "rf1.fit(X_train, y_train)\n",
    "# use the rf object\n",
    "model_1_preds = rf1.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5545da",
   "metadata": {},
   "source": [
    "## 2.\n",
    "Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca547630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 accuracy is: 96.59%\n",
      "\n",
      "confusion matrix for model 1: \n",
      "[[306   1]\n",
      " [ 16 175]]\n",
      "\n",
      "classification report for model 1:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       307\n",
      "           1       0.99      0.92      0.95       191\n",
      "\n",
      "    accuracy                           0.97       498\n",
      "   macro avg       0.97      0.96      0.96       498\n",
      "weighted avg       0.97      0.97      0.97       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 1 score\n",
    "print(f'model 1 accuracy is: {rf1.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 1\n",
    "print(f'confusion matrix for model 1: \\n{confusion_matrix(y_train, model_1_preds)}\\n')\n",
    "\n",
    "# classification report for model 1\n",
    "print(f'classification report for model 1:\\n\\n \\\n",
    "{classification_report(y_train, model_1_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2768b673",
   "metadata": {},
   "source": [
    "## 3.\n",
    "Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc2deacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_model_1 = accuracy_score(y_train, model_1_preds)\n",
    "\n",
    "# true positive rate\n",
    "tp_model_1 = ((y_train == 1) & (model_1_preds == 1)).sum()\n",
    "\n",
    "# false positive rate\n",
    "fp_model_1 = ((y_train == 0) & (model_1_preds == 1)).sum()\n",
    "\n",
    "# true negative rate\n",
    "tn_model_1 = ((y_train == 0) & (model_1_preds == 0)).sum()\n",
    "\n",
    "# false negative rate\n",
    "fn_model_1 = ((y_train == 1) & (model_1_preds == 0)).sum()\n",
    "\n",
    "# f1-score\n",
    "f1_model_1 = f1_score(y_train, model_1_preds)\n",
    "\n",
    "# precision, recall, and support.\n",
    "precision_model_1, recall_model_1, _, support_model_1 = precision_recall_fscore_support(\n",
    "    y_train, model_1_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faba497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for model 1: \t0.9658634538152611\n",
      "true positives for model 1: \t175\n",
      "false positives for model 1: \t1\n",
      "true negatives for model 1: \t306\n",
      "false negatives for model 1: \t16\n",
      "f1_score for model 1: \t0.9536784741144414\n",
      "precision for model 1: \t[0.95031056 0.99431818]\n",
      "recall for model 1: \t[0.99674267 0.91623037]\n",
      "support for model 1: \t[307 191]\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy for model 1: \\t{accuracy_model_1}')\n",
    "print(f'true positives for model 1: \\t{tp_model_1}')\n",
    "print(f'false positives for model 1: \\t{fp_model_1}')\n",
    "print(f'true negatives for model 1: \\t{tn_model_1}')\n",
    "print(f'false negatives for model 1: \\t{fn_model_1}')\n",
    "print(f'f1_score for model 1: \\t{f1_model_1}')\n",
    "print(f'precision for model 1: \\t{precision_model_1}')\n",
    "print(f'recall for model 1: \\t{recall_model_1}')\n",
    "print(f'support for model 1: \\t{support_model_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dcf87b",
   "metadata": {},
   "source": [
    "## 4.\n",
    "Run through steps increasing your min_samples_leaf and decreasing your max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00bbb7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10), (2, 9), (3, 8), (4, 7), (5, 6), (6, 5), (7, 4), (8, 3), (9, 2)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pair for pair in zip(range(1,10), range(10,1,-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d654a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_dict = {}\n",
    "for i in [pair for pair in zip(range(1,10), range(10,1,-1))]:\n",
    "    rf = RandomForestClassifier(min_samples_leaf=i[0],\n",
    "                               max_depth=i[1])\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_model_dict[f'rf_{i[0]}'] = {}\n",
    "    rf_model_dict[f'rf_{i[0]}']['model'] = rf\n",
    "    rf_model_dict[f'rf_{i[0]}']['train_score'] = rf.score(X_train, y_train)\n",
    "    rf_model_dict[f'rf_{i[0]}']['val_score'] = rf.score(X_val, y_val)\n",
    "    rf_model_dict[f'rf_{i[0]}']['val_diff'] = \\\n",
    "    rf.score(X_train, y_train) - rf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "087b8872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf_1': {'model': RandomForestClassifier(max_depth=10),\n",
       "  'train_score': 0.963855421686747,\n",
       "  'val_score': 0.8037383177570093,\n",
       "  'val_diff': 0.16011710392973766},\n",
       " 'rf_2': {'model': RandomForestClassifier(max_depth=9, min_samples_leaf=2),\n",
       "  'train_score': 0.9196787148594378,\n",
       "  'val_score': 0.8084112149532711,\n",
       "  'val_diff': 0.11126749990616669},\n",
       " 'rf_3': {'model': RandomForestClassifier(max_depth=8, min_samples_leaf=3),\n",
       "  'train_score': 0.8955823293172691,\n",
       "  'val_score': 0.8037383177570093,\n",
       "  'val_diff': 0.09184401156025979},\n",
       " 'rf_4': {'model': RandomForestClassifier(max_depth=7, min_samples_leaf=4),\n",
       "  'train_score': 0.8755020080321285,\n",
       "  'val_score': 0.794392523364486,\n",
       "  'val_diff': 0.08110948466764256},\n",
       " 'rf_5': {'model': RandomForestClassifier(max_depth=6, min_samples_leaf=5),\n",
       "  'train_score': 0.8775100401606426,\n",
       "  'val_score': 0.7990654205607477,\n",
       "  'val_diff': 0.0784446195998949},\n",
       " 'rf_6': {'model': RandomForestClassifier(max_depth=5, min_samples_leaf=6),\n",
       "  'train_score': 0.8654618473895582,\n",
       "  'val_score': 0.8130841121495327,\n",
       "  'val_diff': 0.052377735240025536},\n",
       " 'rf_7': {'model': RandomForestClassifier(max_depth=4, min_samples_leaf=7),\n",
       "  'train_score': 0.857429718875502,\n",
       "  'val_score': 0.7990654205607477,\n",
       "  'val_diff': 0.058364298314754315},\n",
       " 'rf_8': {'model': RandomForestClassifier(max_depth=3, min_samples_leaf=8),\n",
       "  'train_score': 0.8413654618473896,\n",
       "  'val_score': 0.7990654205607477,\n",
       "  'val_diff': 0.04230004128664189},\n",
       " 'rf_9': {'model': RandomForestClassifier(max_depth=2, min_samples_leaf=9),\n",
       "  'train_score': 0.785140562248996,\n",
       "  'val_score': 0.7757009345794392,\n",
       "  'val_diff': 0.00943962766955675}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4163707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rf_6'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_score = 0\n",
    "for model in rf_model_dict:\n",
    "    if rf_model_dict[model]['val_score'] > max_score:\n",
    "        max_score = rf_model_dict[model]['val_score']\n",
    "        model_name = model\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5382731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the random forest classifier object\n",
    "# increasing your min_samples_leaf and decreasing your max_depth.\n",
    "rf2 = RandomForestClassifier(max_depth=6, min_samples_leaf=4, random_state=123)\n",
    "# fit the rf object\n",
    "rf2.fit(X_train, y_train)\n",
    "# use the rf object\n",
    "model_2_preds = rf2.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33c3383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 2 accuracy is: 87.35%\n",
      "\n",
      "confusion matrix for model 2: \n",
      "[[296  11]\n",
      " [ 52 139]]\n",
      "\n",
      "classification report for model 2:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       307\n",
      "           1       0.93      0.73      0.82       191\n",
      "\n",
      "    accuracy                           0.87       498\n",
      "   macro avg       0.89      0.85      0.86       498\n",
      "weighted avg       0.88      0.87      0.87       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 2 score\n",
    "print(f'model 2 accuracy is: {rf2.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 2\n",
    "print(f'confusion matrix for model 2: \\n{confusion_matrix(y_train, model_2_preds)}\\n')\n",
    "\n",
    "# classification report for model 2\n",
    "print(f'classification report for model 2:\\n\\n \\\n",
    "{classification_report(y_train, model_2_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5e07043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_model_2 = accuracy_score(y_train, model_2_preds)\n",
    "\n",
    "# true positive rate\n",
    "tp_model_2 = ((y_train == 1) & (model_2_preds == 1)).sum()\n",
    "\n",
    "# false positive rate\n",
    "fp_model_2 = ((y_train == 0) & (model_2_preds == 1)).sum()\n",
    "\n",
    "# true negative rate\n",
    "tn_model_2 = ((y_train == 0) & (model_2_preds == 0)).sum()\n",
    "\n",
    "# false negative rate\n",
    "fn_model_2 = ((y_train == 1) & (model_2_preds == 0)).sum()\n",
    "\n",
    "# f1-score\n",
    "f1_model_2 = f1_score(y_train, model_2_preds)\n",
    "\n",
    "# precision, recall, and support.\n",
    "precision_model_2, recall_model_2, _, support_model_2 = precision_recall_fscore_support(\n",
    "    y_train, model_2_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dcfe80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for model 2: \t0.8734939759036144\n",
      "true positives for model 2: \t139\n",
      "false positives for model 2: \t11\n",
      "true negatives for model 2: \t296\n",
      "false negatives for model 2: \t52\n",
      "f1_score for model 2: \t0.81524926686217\n",
      "precision for model 2: \t[0.85057471 0.92666667]\n",
      "recall for model 2: \t[0.96416938 0.72774869]\n",
      "support for model 2: \t[307 191]\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy for model 2: \\t{accuracy_model_2}')\n",
    "print(f'true positives for model 2: \\t{tp_model_2}')\n",
    "print(f'false positives for model 2: \\t{fp_model_2}')\n",
    "print(f'true negatives for model 2: \\t{tn_model_2}')\n",
    "print(f'false negatives for model 2: \\t{fn_model_2}')\n",
    "print(f'f1_score for model 2: \\t{f1_model_2}')\n",
    "print(f'precision for model 2: \\t{precision_model_2}')\n",
    "print(f'recall for model 2: \\t{recall_model_2}')\n",
    "print(f'support for model 2: \\t{support_model_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc813cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the random forest classifier object\n",
    "# increasing your min_samples_leaf and decreasing your max_depth.\n",
    "rf3 = RandomForestClassifier(max_depth=5, min_samples_leaf=5, random_state=123)\n",
    "# fit the rf object\n",
    "rf3.fit(X_train, y_train)\n",
    "# use the rf object\n",
    "model_3_preds = rf3.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9fd73da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 3 accuracy is: 86.35%\n",
      "\n",
      "confusion matrix for model 3: \n",
      "[[290  17]\n",
      " [ 51 140]]\n",
      "\n",
      "classification report for model 3:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.90       307\n",
      "           1       0.89      0.73      0.80       191\n",
      "\n",
      "    accuracy                           0.86       498\n",
      "   macro avg       0.87      0.84      0.85       498\n",
      "weighted avg       0.87      0.86      0.86       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 3 score\n",
    "print(f'model 3 accuracy is: {rf3.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 3\n",
    "print(f'confusion matrix for model 3: \\n{confusion_matrix(y_train, model_3_preds)}\\n')\n",
    "\n",
    "# classification report for model 3\n",
    "print(f'classification report for model 3:\\n\\n \\\n",
    "{classification_report(y_train, model_3_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57d9823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_model_3 = accuracy_score(y_train, model_3_preds)\n",
    "\n",
    "# true positive rate\n",
    "tp_model_3 = ((y_train == 1) & (model_3_preds == 1)).sum()\n",
    "\n",
    "# false positive rate\n",
    "fp_model_3 = ((y_train == 0) & (model_3_preds == 1)).sum()\n",
    "\n",
    "# true negative rate\n",
    "tn_model_3 = ((y_train == 0) & (model_3_preds == 0)).sum()\n",
    "\n",
    "# false negative rate\n",
    "fn_model_3 = ((y_train == 1) & (model_3_preds == 0)).sum()\n",
    "\n",
    "# f1-score\n",
    "f1_model_3 = f1_score(y_train, model_3_preds)\n",
    "\n",
    "# precision, recall, and support.\n",
    "precision_model_3, recall_model_3, _, support_model_3 = precision_recall_fscore_support(\n",
    "    y_train, model_3_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70b3adb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for model 3: \t0.8634538152610441\n",
      "true positives for model 3: \t140\n",
      "false positives for model 3: \t17\n",
      "true negatives for model 3: \t290\n",
      "false negatives for model 3: \t51\n",
      "f1_score for model 3: \t0.8045977011494254\n",
      "precision for model 3: \t[0.85043988 0.89171975]\n",
      "recall for model 3: \t[0.94462541 0.73298429]\n",
      "support for model 3: \t[307 191]\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy for model 3: \\t{accuracy_model_3}')\n",
    "print(f'true positives for model 3: \\t{tp_model_3}')\n",
    "print(f'false positives for model 3: \\t{fp_model_3}')\n",
    "print(f'true negatives for model 3: \\t{tn_model_3}')\n",
    "print(f'false negatives for model 3: \\t{fn_model_3}')\n",
    "print(f'f1_score for model 3: \\t{f1_model_3}')\n",
    "print(f'precision for model 3: \\t{precision_model_3}')\n",
    "print(f'recall for model 3: \\t{recall_model_3}')\n",
    "print(f'support for model 3: \\t{support_model_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde6d05d",
   "metadata": {},
   "source": [
    "## 5.\n",
    "What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "613f376b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 accuracy is: 96.59%\n",
      "\n",
      "confusion matrix for model 1: \n",
      "[[306   1]\n",
      " [ 16 175]]\n",
      "\n",
      "classification report for model 1:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       307\n",
      "           1       0.99      0.92      0.95       191\n",
      "\n",
      "    accuracy                           0.97       498\n",
      "   macro avg       0.97      0.96      0.96       498\n",
      "weighted avg       0.97      0.97      0.97       498\n",
      "\n",
      "model 2 accuracy is: 87.35%\n",
      "\n",
      "confusion matrix for model 2: \n",
      "[[296  11]\n",
      " [ 52 139]]\n",
      "\n",
      "classification report for model 2:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       307\n",
      "           1       0.93      0.73      0.82       191\n",
      "\n",
      "    accuracy                           0.87       498\n",
      "   macro avg       0.89      0.85      0.86       498\n",
      "weighted avg       0.88      0.87      0.87       498\n",
      "\n",
      "model 3 accuracy is: 86.35%\n",
      "\n",
      "confusion matrix for model 3: \n",
      "[[290  17]\n",
      " [ 51 140]]\n",
      "\n",
      "classification report for model 3:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.90       307\n",
      "           1       0.89      0.73      0.80       191\n",
      "\n",
      "    accuracy                           0.86       498\n",
      "   macro avg       0.87      0.84      0.85       498\n",
      "weighted avg       0.87      0.86      0.86       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 1 score\n",
    "print(f'model 1 accuracy is: {rf1.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 1\n",
    "print(f'confusion matrix for model 1: \\n{confusion_matrix(y_train, model_1_preds)}\\n')\n",
    "\n",
    "# classification report for model 1\n",
    "print(f'classification report for model 1:\\n\\n \\\n",
    "{classification_report(y_train, model_1_preds)}')\n",
    "\n",
    "# model 2 score\n",
    "print(f'model 2 accuracy is: {rf2.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 2\n",
    "print(f'confusion matrix for model 2: \\n{confusion_matrix(y_train, model_2_preds)}\\n')\n",
    "\n",
    "# classification report for model 2\n",
    "print(f'classification report for model 2:\\n\\n \\\n",
    "{classification_report(y_train, model_2_preds)}')\n",
    "\n",
    "# model 3 score\n",
    "print(f'model 3 accuracy is: {rf3.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 3\n",
    "print(f'confusion matrix for model 3: \\n{confusion_matrix(y_train, model_3_preds)}\\n')\n",
    "\n",
    "# classification report for model 3\n",
    "print(f'classification report for model 3:\\n\\n \\\n",
    "{classification_report(y_train, model_3_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b887352",
   "metadata": {},
   "source": [
    "The first model with the highest depth and lowest min_sample_leaf appears to be the best performing model based on the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9df06d7",
   "metadata": {},
   "source": [
    "## 6. \n",
    "After making a few models, which one has the best performance (or closest metrics) on both train and validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe12da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = validate[X_cols]\n",
    "y_val = validate[y_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f320b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_1_preds = rf1.predict(X_val)\n",
    "val_2_preds = rf2.predict(X_val)\n",
    "val_3_preds = rf3.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8422b909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 accuracy using the validation data is: 79.44%\n",
      "\n",
      "confusion matrix for model 1 using the validation data: \n",
      "[[110  22]\n",
      " [ 22  60]]\n",
      "\n",
      "classification report for model 1 using the validation data:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83       132\n",
      "           1       0.73      0.73      0.73        82\n",
      "\n",
      "    accuracy                           0.79       214\n",
      "   macro avg       0.78      0.78      0.78       214\n",
      "weighted avg       0.79      0.79      0.79       214\n",
      "\n",
      "model 2 accuracy using the validation data is: 80.37%\n",
      "\n",
      "confusion matrix for model 2 using the validation data: \n",
      "[[118  14]\n",
      " [ 28  54]]\n",
      "\n",
      "classification report for model 2 using the validation data:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85       132\n",
      "           1       0.79      0.66      0.72        82\n",
      "\n",
      "    accuracy                           0.80       214\n",
      "   macro avg       0.80      0.78      0.78       214\n",
      "weighted avg       0.80      0.80      0.80       214\n",
      "\n",
      "model 3 accuracy using the validation data is: 79.91%\n",
      "\n",
      "confusion matrix for model 3 using the validation data: \n",
      "[[116  16]\n",
      " [ 27  55]]\n",
      "\n",
      "classification report for model 3 using the validation data:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       132\n",
      "           1       0.77      0.67      0.72        82\n",
      "\n",
      "    accuracy                           0.80       214\n",
      "   macro avg       0.79      0.77      0.78       214\n",
      "weighted avg       0.80      0.80      0.80       214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 1 score\n",
    "print(f'model 1 accuracy using the validation data is: {rf1.score(X_val, y_val):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 1\n",
    "print(f'confusion matrix for model 1 using the validation data: \\n{confusion_matrix(y_val, val_1_preds)}\\n')\n",
    "\n",
    "# classification report for model 1\n",
    "print(f'classification report for model 1 using the validation data:\\n\\n \\\n",
    "{classification_report(y_val, val_1_preds)}')\n",
    "\n",
    "# model 2 score\n",
    "print(f'model 2 accuracy using the validation data is: {rf2.score(X_val, y_val):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 2\n",
    "print(f'confusion matrix for model 2 using the validation data: \\n{confusion_matrix(y_val, val_2_preds)}\\n')\n",
    "\n",
    "# classification report for model 2\n",
    "print(f'classification report for model 2 using the validation data:\\n\\n \\\n",
    "{classification_report(y_val, val_2_preds)}')\n",
    "\n",
    "# model 3 score\n",
    "print(f'model 3 accuracy using the validation data is: {rf3.score(X_val, y_val):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 3\n",
    "print(f'confusion matrix for model 3 using the validation data: \\n{confusion_matrix(y_val, val_3_preds)}\\n')\n",
    "\n",
    "# classification report for model 3\n",
    "print(f'classification report for model 3 using the validation data:\\n\\n \\\n",
    "{classification_report(y_val, val_3_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b824b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.97999999999999"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "87.35 - 80.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f446a1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.439999999999998"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "86.35 - 79.91"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc51e06",
   "metadata": {},
   "source": [
    "Model 3 with max_depth=5, min_samples_leaf=5 had the lowest drop in accuracy from the training data and the validation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
