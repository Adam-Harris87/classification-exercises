{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41646bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,\\\n",
    "f1_score, precision_recall_fscore_support\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a4ab4",
   "metadata": {},
   "source": [
    "## 1.\n",
    "Fit the Random Forest classifier to your training sample and transform (i.e. make predictions on the training sample) setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a8ebd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = prepare.wrangle_data('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd95961",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = train.drop(columns=['sex','embark_town','survived']).columns.to_list()\n",
    "y_cols = 'survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e162cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[X_cols]\n",
    "y_train = train[y_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9a967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the random forest classifier object\n",
    "# setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10.\n",
    "rf1 = RandomForestClassifier(max_depth=10, min_samples_leaf=1, random_state=123)\n",
    "# fit the rf object\n",
    "rf1.fit(X_train, y_train)\n",
    "# use the rf object\n",
    "model_1_preds = rf1.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d7af4",
   "metadata": {},
   "source": [
    "## 2.\n",
    "Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08848912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 accuracy is: 96.59%\n",
      "\n",
      "confusion matrix for model 1: \n",
      "[[306   1]\n",
      " [ 16 175]]\n",
      "\n",
      "classification report for model 1:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       307\n",
      "           1       0.99      0.92      0.95       191\n",
      "\n",
      "    accuracy                           0.97       498\n",
      "   macro avg       0.97      0.96      0.96       498\n",
      "weighted avg       0.97      0.97      0.97       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 1 score\n",
    "print(f'model 1 accuracy is: {rf1.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 1\n",
    "print(f'confusion matrix for model 1: \\n{confusion_matrix(y_train, model_1_preds)}\\n')\n",
    "\n",
    "# classification report for model 1\n",
    "print(f'classification report for model 1:\\n\\n \\\n",
    "{classification_report(y_train, model_1_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708adfc",
   "metadata": {},
   "source": [
    "## 3.\n",
    "Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a2ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_model_1 = accuracy_score(y_train, model_1_preds)\n",
    "\n",
    "# true positive rate\n",
    "tp_model_1 = ((y_train == 1) & (model_1_preds == 1)).sum()\n",
    "\n",
    "# false positive rate\n",
    "fp_model_1 = ((y_train == 0) & (model_1_preds == 1)).sum()\n",
    "\n",
    "# true negative rate\n",
    "tn_model_1 = ((y_train == 0) & (model_1_preds == 0)).sum()\n",
    "\n",
    "# false negative rate\n",
    "fn_model_1 = ((y_train == 1) & (model_1_preds == 0)).sum()\n",
    "\n",
    "# f1-score\n",
    "f1_model_1 = f1_score(y_train, model_1_preds)\n",
    "\n",
    "# precision, recall, and support.\n",
    "precision_model_1, recall_model_1, _, support_model_1 = precision_recall_fscore_support(\n",
    "    y_train, model_1_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6d57a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for model 1: \t0.9658634538152611\n",
      "true positives for model 1: \t175\n",
      "false positives for model 1: \t1\n",
      "true negatives for model 1: \t306\n",
      "false negatives for model 1: \t16\n",
      "f1_score for model 1: \t0.9536784741144414\n",
      "precision for model 1: \t[0.95031056 0.99431818]\n",
      "recall for model 1: \t[0.99674267 0.91623037]\n",
      "support for model 1: \t[307 191]\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy for model 1: \\t{accuracy_model_1}')\n",
    "print(f'true positives for model 1: \\t{tp_model_1}')\n",
    "print(f'false positives for model 1: \\t{fp_model_1}')\n",
    "print(f'true negatives for model 1: \\t{tn_model_1}')\n",
    "print(f'false negatives for model 1: \\t{fn_model_1}')\n",
    "print(f'f1_score for model 1: \\t{f1_model_1}')\n",
    "print(f'precision for model 1: \\t{precision_model_1}')\n",
    "print(f'recall for model 1: \\t{recall_model_1}')\n",
    "print(f'support for model 1: \\t{support_model_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25912a4",
   "metadata": {},
   "source": [
    "## 4.\n",
    "Run through steps increasing your min_samples_leaf and decreasing your max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2280bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the random forest classifier object\n",
    "# increasing your min_samples_leaf and decreasing your max_depth.\n",
    "rf2 = RandomForestClassifier(max_depth=6, min_samples_leaf=4, random_state=123)\n",
    "# fit the rf object\n",
    "rf2.fit(X_train, y_train)\n",
    "# use the rf object\n",
    "model_2_preds = rf2.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8798a9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 2 accuracy is: 87.35%\n",
      "\n",
      "confusion matrix for model 2: \n",
      "[[296  11]\n",
      " [ 52 139]]\n",
      "\n",
      "classification report for model 2:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       307\n",
      "           1       0.93      0.73      0.82       191\n",
      "\n",
      "    accuracy                           0.87       498\n",
      "   macro avg       0.89      0.85      0.86       498\n",
      "weighted avg       0.88      0.87      0.87       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 2 score\n",
    "print(f'model 2 accuracy is: {rf2.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 2\n",
    "print(f'confusion matrix for model 2: \\n{confusion_matrix(y_train, model_2_preds)}\\n')\n",
    "\n",
    "# classification report for model 2\n",
    "print(f'classification report for model 2:\\n\\n \\\n",
    "{classification_report(y_train, model_2_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a811b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_model_2 = accuracy_score(y_train, model_2_preds)\n",
    "\n",
    "# true positive rate\n",
    "tp_model_2 = ((y_train == 1) & (model_2_preds == 1)).sum()\n",
    "\n",
    "# false positive rate\n",
    "fp_model_2 = ((y_train == 0) & (model_2_preds == 1)).sum()\n",
    "\n",
    "# true negative rate\n",
    "tn_model_2 = ((y_train == 0) & (model_2_preds == 0)).sum()\n",
    "\n",
    "# false negative rate\n",
    "fn_model_2 = ((y_train == 1) & (model_2_preds == 0)).sum()\n",
    "\n",
    "# f1-score\n",
    "f1_model_2 = f1_score(y_train, model_2_preds)\n",
    "\n",
    "# precision, recall, and support.\n",
    "precision_model_2, recall_model_2, _, support_model_2 = precision_recall_fscore_support(\n",
    "    y_train, model_2_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fda8c6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for model 2: \t0.8734939759036144\n",
      "true positives for model 2: \t139\n",
      "false positives for model 2: \t11\n",
      "true negatives for model 2: \t296\n",
      "false negatives for model 2: \t52\n",
      "f1_score for model 2: \t0.81524926686217\n",
      "precision for model 2: \t[0.85057471 0.92666667]\n",
      "recall for model 2: \t[0.96416938 0.72774869]\n",
      "support for model 2: \t[307 191]\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy for model 2: \\t{accuracy_model_2}')\n",
    "print(f'true positives for model 2: \\t{tp_model_2}')\n",
    "print(f'false positives for model 2: \\t{fp_model_2}')\n",
    "print(f'true negatives for model 2: \\t{tn_model_2}')\n",
    "print(f'false negatives for model 2: \\t{fn_model_2}')\n",
    "print(f'f1_score for model 2: \\t{f1_model_2}')\n",
    "print(f'precision for model 2: \\t{precision_model_2}')\n",
    "print(f'recall for model 2: \\t{recall_model_2}')\n",
    "print(f'support for model 2: \\t{support_model_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "603ea176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the random forest classifier object\n",
    "# increasing your min_samples_leaf and decreasing your max_depth.\n",
    "rf3 = RandomForestClassifier(max_depth=5, min_samples_leaf=5, random_state=123)\n",
    "# fit the rf object\n",
    "rf3.fit(X_train, y_train)\n",
    "# use the rf object\n",
    "model_3_preds = rf3.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ff22f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 3 accuracy is: 86.35%\n",
      "\n",
      "confusion matrix for model 3: \n",
      "[[290  17]\n",
      " [ 51 140]]\n",
      "\n",
      "classification report for model 3:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.90       307\n",
      "           1       0.89      0.73      0.80       191\n",
      "\n",
      "    accuracy                           0.86       498\n",
      "   macro avg       0.87      0.84      0.85       498\n",
      "weighted avg       0.87      0.86      0.86       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 3 score\n",
    "print(f'model 3 accuracy is: {rf3.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 3\n",
    "print(f'confusion matrix for model 3: \\n{confusion_matrix(y_train, model_3_preds)}\\n')\n",
    "\n",
    "# classification report for model 3\n",
    "print(f'classification report for model 3:\\n\\n \\\n",
    "{classification_report(y_train, model_3_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "844a7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_model_3 = accuracy_score(y_train, model_3_preds)\n",
    "\n",
    "# true positive rate\n",
    "tp_model_3 = ((y_train == 1) & (model_3_preds == 1)).sum()\n",
    "\n",
    "# false positive rate\n",
    "fp_model_3 = ((y_train == 0) & (model_3_preds == 1)).sum()\n",
    "\n",
    "# true negative rate\n",
    "tn_model_3 = ((y_train == 0) & (model_3_preds == 0)).sum()\n",
    "\n",
    "# false negative rate\n",
    "fn_model_3 = ((y_train == 1) & (model_3_preds == 0)).sum()\n",
    "\n",
    "# f1-score\n",
    "f1_model_3 = f1_score(y_train, model_3_preds)\n",
    "\n",
    "# precision, recall, and support.\n",
    "precision_model_3, recall_model_3, _, support_model_3 = precision_recall_fscore_support(\n",
    "    y_train, model_3_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7133c1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for model 3: \t0.8634538152610441\n",
      "true positives for model 3: \t140\n",
      "false positives for model 3: \t17\n",
      "true negatives for model 3: \t290\n",
      "false negatives for model 3: \t51\n",
      "f1_score for model 3: \t0.8045977011494254\n",
      "precision for model 3: \t[0.85043988 0.89171975]\n",
      "recall for model 3: \t[0.94462541 0.73298429]\n",
      "support for model 3: \t[307 191]\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy for model 3: \\t{accuracy_model_3}')\n",
    "print(f'true positives for model 3: \\t{tp_model_3}')\n",
    "print(f'false positives for model 3: \\t{fp_model_3}')\n",
    "print(f'true negatives for model 3: \\t{tn_model_3}')\n",
    "print(f'false negatives for model 3: \\t{fn_model_3}')\n",
    "print(f'f1_score for model 3: \\t{f1_model_3}')\n",
    "print(f'precision for model 3: \\t{precision_model_3}')\n",
    "print(f'recall for model 3: \\t{recall_model_3}')\n",
    "print(f'support for model 3: \\t{support_model_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1f672",
   "metadata": {},
   "source": [
    "## 5.\n",
    "What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9a8d871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 accuracy is: 96.59%\n",
      "\n",
      "confusion matrix for model 1: \n",
      "[[306   1]\n",
      " [ 16 175]]\n",
      "\n",
      "classification report for model 1:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       307\n",
      "           1       0.99      0.92      0.95       191\n",
      "\n",
      "    accuracy                           0.97       498\n",
      "   macro avg       0.97      0.96      0.96       498\n",
      "weighted avg       0.97      0.97      0.97       498\n",
      "\n",
      "model 2 accuracy is: 87.35%\n",
      "\n",
      "confusion matrix for model 2: \n",
      "[[296  11]\n",
      " [ 52 139]]\n",
      "\n",
      "classification report for model 2:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       307\n",
      "           1       0.93      0.73      0.82       191\n",
      "\n",
      "    accuracy                           0.87       498\n",
      "   macro avg       0.89      0.85      0.86       498\n",
      "weighted avg       0.88      0.87      0.87       498\n",
      "\n",
      "model 3 accuracy is: 86.35%\n",
      "\n",
      "confusion matrix for model 3: \n",
      "[[290  17]\n",
      " [ 51 140]]\n",
      "\n",
      "classification report for model 3:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.90       307\n",
      "           1       0.89      0.73      0.80       191\n",
      "\n",
      "    accuracy                           0.86       498\n",
      "   macro avg       0.87      0.84      0.85       498\n",
      "weighted avg       0.87      0.86      0.86       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 1 score\n",
    "print(f'model 1 accuracy is: {rf1.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 1\n",
    "print(f'confusion matrix for model 1: \\n{confusion_matrix(y_train, model_1_preds)}\\n')\n",
    "\n",
    "# classification report for model 1\n",
    "print(f'classification report for model 1:\\n\\n \\\n",
    "{classification_report(y_train, model_1_preds)}')\n",
    "\n",
    "# model 2 score\n",
    "print(f'model 2 accuracy is: {rf2.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 2\n",
    "print(f'confusion matrix for model 2: \\n{confusion_matrix(y_train, model_2_preds)}\\n')\n",
    "\n",
    "# classification report for model 2\n",
    "print(f'classification report for model 2:\\n\\n \\\n",
    "{classification_report(y_train, model_2_preds)}')\n",
    "\n",
    "# model 3 score\n",
    "print(f'model 3 accuracy is: {rf3.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 3\n",
    "print(f'confusion matrix for model 3: \\n{confusion_matrix(y_train, model_3_preds)}\\n')\n",
    "\n",
    "# classification report for model 3\n",
    "print(f'classification report for model 3:\\n\\n \\\n",
    "{classification_report(y_train, model_3_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2fb3d",
   "metadata": {},
   "source": [
    "The first model with the highest depth and lowest min_sample_leaf appears to be the best performing model based on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d371d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
