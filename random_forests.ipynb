{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b71ab139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,\\\n",
    "f1_score, precision_recall_fscore_support\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f35a7",
   "metadata": {},
   "source": [
    "## 1.\n",
    "Fit the Random Forest classifier to your training sample and transform (i.e. make predictions on the training sample) setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a069d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = prepare.wrangle_data('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57082ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = train.drop(columns=['sex','embark_town','survived']).columns.to_list()\n",
    "y_cols = 'survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c43a4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[X_cols]\n",
    "y_train = train[y_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "464ea3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the random forest classifier object\n",
    "# setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10.\n",
    "rf1 = RandomForestClassifier(max_depth=10, min_samples_leaf=1, random_state=123)\n",
    "# fit the rf object\n",
    "rf1.fit(X_train, y_train)\n",
    "# use the rf object\n",
    "model_1_preds = rf1.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952acbc",
   "metadata": {},
   "source": [
    "## 2.\n",
    "Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b769d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 accuracy is: 96.59%\n",
      "\n",
      "confusion matrix for model 1: \n",
      "[[306   1]\n",
      " [ 16 175]]\n",
      "\n",
      "classification report for model 1:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       307\n",
      "           1       0.99      0.92      0.95       191\n",
      "\n",
      "    accuracy                           0.97       498\n",
      "   macro avg       0.97      0.96      0.96       498\n",
      "weighted avg       0.97      0.97      0.97       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 1 score\n",
    "print(f'model 1 accuracy is: {rf1.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 1\n",
    "print(f'confusion matrix for model 1: \\n{confusion_matrix(y_train, model_1_preds)}\\n')\n",
    "\n",
    "# classification report for model 1\n",
    "print(f'classification report for model 1:\\n\\n \\\n",
    "{classification_report(y_train, model_1_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898c207",
   "metadata": {},
   "source": [
    "## 3.\n",
    "Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87eed07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_model_1 = accuracy_score(y_train, model_1_preds)\n",
    "\n",
    "# true positive rate\n",
    "tp_model_1 = ((y_train == 1) & (model_1_preds == 1)).sum()\n",
    "\n",
    "# false positive rate\n",
    "fp_model_1 = ((y_train == 0) & (model_1_preds == 1)).sum()\n",
    "\n",
    "# true negative rate\n",
    "tn_model_1 = ((y_train == 0) & (model_1_preds == 0)).sum()\n",
    "\n",
    "# false negative rate\n",
    "fn_model_1 = ((y_train == 1) & (model_1_preds == 0)).sum()\n",
    "\n",
    "# f1-score\n",
    "f1_model_1 = f1_score(y_train, model_1_preds)\n",
    "\n",
    "# precision, recall, and support.\n",
    "precision_model_1, recall_model_1, _, support_model_1 = precision_recall_fscore_support(\n",
    "    y_train, model_1_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb6f4271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for model 1: \t0.9658634538152611\n",
      "true positives for model 1: \t175\n",
      "false positives for model 1: \t1\n",
      "true negatives for model 1: \t306\n",
      "false negatives for model 1: \t16\n",
      "f1_score for model 1: \t0.9536784741144414\n",
      "precision for model 1: \t[0.95031056 0.99431818]\n",
      "recall for model 1: \t[0.99674267 0.91623037]\n",
      "support for model 1: \t[307 191]\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy for model 1: \\t{accuracy_model_1}')\n",
    "print(f'true positives for model 1: \\t{tp_model_1}')\n",
    "print(f'false positives for model 1: \\t{fp_model_1}')\n",
    "print(f'true negatives for model 1: \\t{tn_model_1}')\n",
    "print(f'false negatives for model 1: \\t{fn_model_1}')\n",
    "print(f'f1_score for model 1: \\t{f1_model_1}')\n",
    "print(f'precision for model 1: \\t{precision_model_1}')\n",
    "print(f'recall for model 1: \\t{recall_model_1}')\n",
    "print(f'support for model 1: \\t{support_model_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16e5b3",
   "metadata": {},
   "source": [
    "## 4.\n",
    "Run through steps increasing your min_samples_leaf and decreasing your max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f0b759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the random forest classifier object\n",
    "# increasing your min_samples_leaf and decreasing your max_depth.\n",
    "rf2 = RandomForestClassifier(max_depth=6, min_samples_leaf=4, random_state=123)\n",
    "# fit the rf object\n",
    "rf2.fit(X_train, y_train)\n",
    "# use the rf object\n",
    "model_2_preds = rf2.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d3aca5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 2 accuracy is: 87.35%\n",
      "\n",
      "confusion matrix for model 2: \n",
      "[[296  11]\n",
      " [ 52 139]]\n",
      "\n",
      "classification report for model 2:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       307\n",
      "           1       0.93      0.73      0.82       191\n",
      "\n",
      "    accuracy                           0.87       498\n",
      "   macro avg       0.89      0.85      0.86       498\n",
      "weighted avg       0.88      0.87      0.87       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 2 score\n",
    "print(f'model 2 accuracy is: {rf2.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 2\n",
    "print(f'confusion matrix for model 2: \\n{confusion_matrix(y_train, model_2_preds)}\\n')\n",
    "\n",
    "# classification report for model 2\n",
    "print(f'classification report for model 2:\\n\\n \\\n",
    "{classification_report(y_train, model_2_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a212629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_model_2 = accuracy_score(y_train, model_2_preds)\n",
    "\n",
    "# true positive rate\n",
    "tp_model_2 = ((y_train == 1) & (model_2_preds == 1)).sum()\n",
    "\n",
    "# false positive rate\n",
    "fp_model_2 = ((y_train == 0) & (model_2_preds == 1)).sum()\n",
    "\n",
    "# true negative rate\n",
    "tn_model_2 = ((y_train == 0) & (model_2_preds == 0)).sum()\n",
    "\n",
    "# false negative rate\n",
    "fn_model_2 = ((y_train == 1) & (model_2_preds == 0)).sum()\n",
    "\n",
    "# f1-score\n",
    "f1_model_2 = f1_score(y_train, model_2_preds)\n",
    "\n",
    "# precision, recall, and support.\n",
    "precision_model_2, recall_model_2, _, support_model_2 = precision_recall_fscore_support(\n",
    "    y_train, model_2_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59a5ddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for model 2: \t0.8734939759036144\n",
      "true positives for model 2: \t139\n",
      "false positives for model 2: \t11\n",
      "true negatives for model 2: \t296\n",
      "false negatives for model 2: \t52\n",
      "f1_score for model 2: \t0.81524926686217\n",
      "precision for model 2: \t[0.85057471 0.92666667]\n",
      "recall for model 2: \t[0.96416938 0.72774869]\n",
      "support for model 2: \t[307 191]\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy for model 2: \\t{accuracy_model_2}')\n",
    "print(f'true positives for model 2: \\t{tp_model_2}')\n",
    "print(f'false positives for model 2: \\t{fp_model_2}')\n",
    "print(f'true negatives for model 2: \\t{tn_model_2}')\n",
    "print(f'false negatives for model 2: \\t{fn_model_2}')\n",
    "print(f'f1_score for model 2: \\t{f1_model_2}')\n",
    "print(f'precision for model 2: \\t{precision_model_2}')\n",
    "print(f'recall for model 2: \\t{recall_model_2}')\n",
    "print(f'support for model 2: \\t{support_model_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5b2cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the random forest classifier object\n",
    "# increasing your min_samples_leaf and decreasing your max_depth.\n",
    "rf3 = RandomForestClassifier(max_depth=5, min_samples_leaf=5, random_state=123)\n",
    "# fit the rf object\n",
    "rf3.fit(X_train, y_train)\n",
    "# use the rf object\n",
    "model_3_preds = rf3.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6865853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 3 accuracy is: 86.35%\n",
      "\n",
      "confusion matrix for model 3: \n",
      "[[290  17]\n",
      " [ 51 140]]\n",
      "\n",
      "classification report for model 3:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.90       307\n",
      "           1       0.89      0.73      0.80       191\n",
      "\n",
      "    accuracy                           0.86       498\n",
      "   macro avg       0.87      0.84      0.85       498\n",
      "weighted avg       0.87      0.86      0.86       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 3 score\n",
    "print(f'model 3 accuracy is: {rf3.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 3\n",
    "print(f'confusion matrix for model 3: \\n{confusion_matrix(y_train, model_3_preds)}\\n')\n",
    "\n",
    "# classification report for model 3\n",
    "print(f'classification report for model 3:\\n\\n \\\n",
    "{classification_report(y_train, model_3_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e66e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_model_3 = accuracy_score(y_train, model_3_preds)\n",
    "\n",
    "# true positive rate\n",
    "tp_model_3 = ((y_train == 1) & (model_3_preds == 1)).sum()\n",
    "\n",
    "# false positive rate\n",
    "fp_model_3 = ((y_train == 0) & (model_3_preds == 1)).sum()\n",
    "\n",
    "# true negative rate\n",
    "tn_model_3 = ((y_train == 0) & (model_3_preds == 0)).sum()\n",
    "\n",
    "# false negative rate\n",
    "fn_model_3 = ((y_train == 1) & (model_3_preds == 0)).sum()\n",
    "\n",
    "# f1-score\n",
    "f1_model_3 = f1_score(y_train, model_3_preds)\n",
    "\n",
    "# precision, recall, and support.\n",
    "precision_model_3, recall_model_3, _, support_model_3 = precision_recall_fscore_support(\n",
    "    y_train, model_3_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af399584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for model 3: \t0.8634538152610441\n",
      "true positives for model 3: \t140\n",
      "false positives for model 3: \t17\n",
      "true negatives for model 3: \t290\n",
      "false negatives for model 3: \t51\n",
      "f1_score for model 3: \t0.8045977011494254\n",
      "precision for model 3: \t[0.85043988 0.89171975]\n",
      "recall for model 3: \t[0.94462541 0.73298429]\n",
      "support for model 3: \t[307 191]\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy for model 3: \\t{accuracy_model_3}')\n",
    "print(f'true positives for model 3: \\t{tp_model_3}')\n",
    "print(f'false positives for model 3: \\t{fp_model_3}')\n",
    "print(f'true negatives for model 3: \\t{tn_model_3}')\n",
    "print(f'false negatives for model 3: \\t{fn_model_3}')\n",
    "print(f'f1_score for model 3: \\t{f1_model_3}')\n",
    "print(f'precision for model 3: \\t{precision_model_3}')\n",
    "print(f'recall for model 3: \\t{recall_model_3}')\n",
    "print(f'support for model 3: \\t{support_model_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecee6b",
   "metadata": {},
   "source": [
    "## 5.\n",
    "What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c2ffbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 accuracy is: 96.59%\n",
      "\n",
      "confusion matrix for model 1: \n",
      "[[306   1]\n",
      " [ 16 175]]\n",
      "\n",
      "classification report for model 1:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       307\n",
      "           1       0.99      0.92      0.95       191\n",
      "\n",
      "    accuracy                           0.97       498\n",
      "   macro avg       0.97      0.96      0.96       498\n",
      "weighted avg       0.97      0.97      0.97       498\n",
      "\n",
      "model 2 accuracy is: 87.35%\n",
      "\n",
      "confusion matrix for model 2: \n",
      "[[296  11]\n",
      " [ 52 139]]\n",
      "\n",
      "classification report for model 2:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       307\n",
      "           1       0.93      0.73      0.82       191\n",
      "\n",
      "    accuracy                           0.87       498\n",
      "   macro avg       0.89      0.85      0.86       498\n",
      "weighted avg       0.88      0.87      0.87       498\n",
      "\n",
      "model 3 accuracy is: 86.35%\n",
      "\n",
      "confusion matrix for model 3: \n",
      "[[290  17]\n",
      " [ 51 140]]\n",
      "\n",
      "classification report for model 3:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.90       307\n",
      "           1       0.89      0.73      0.80       191\n",
      "\n",
      "    accuracy                           0.86       498\n",
      "   macro avg       0.87      0.84      0.85       498\n",
      "weighted avg       0.87      0.86      0.86       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 1 score\n",
    "print(f'model 1 accuracy is: {rf1.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 1\n",
    "print(f'confusion matrix for model 1: \\n{confusion_matrix(y_train, model_1_preds)}\\n')\n",
    "\n",
    "# classification report for model 1\n",
    "print(f'classification report for model 1:\\n\\n \\\n",
    "{classification_report(y_train, model_1_preds)}')\n",
    "\n",
    "# model 2 score\n",
    "print(f'model 2 accuracy is: {rf2.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 2\n",
    "print(f'confusion matrix for model 2: \\n{confusion_matrix(y_train, model_2_preds)}\\n')\n",
    "\n",
    "# classification report for model 2\n",
    "print(f'classification report for model 2:\\n\\n \\\n",
    "{classification_report(y_train, model_2_preds)}')\n",
    "\n",
    "# model 3 score\n",
    "print(f'model 3 accuracy is: {rf3.score(X_train, y_train):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 3\n",
    "print(f'confusion matrix for model 3: \\n{confusion_matrix(y_train, model_3_preds)}\\n')\n",
    "\n",
    "# classification report for model 3\n",
    "print(f'classification report for model 3:\\n\\n \\\n",
    "{classification_report(y_train, model_3_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77332765",
   "metadata": {},
   "source": [
    "The first model with the highest depth and lowest min_sample_leaf appears to be the best performing model based on the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee4650",
   "metadata": {},
   "source": [
    "## 6. \n",
    "After making a few models, which one has the best performance (or closest metrics) on both train and validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96bc1dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = validate[X_cols]\n",
    "y_val = validate[y_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be0c7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_1_preds = rf1.predict(X_val)\n",
    "val_2_preds = rf2.predict(X_val)\n",
    "val_3_preds = rf3.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ceacdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 accuracy using the validation data is: 79.44%\n",
      "\n",
      "confusion matrix for model 1 using the validation data: \n",
      "[[110  22]\n",
      " [ 22  60]]\n",
      "\n",
      "classification report for model 1 using the validation data:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83       132\n",
      "           1       0.73      0.73      0.73        82\n",
      "\n",
      "    accuracy                           0.79       214\n",
      "   macro avg       0.78      0.78      0.78       214\n",
      "weighted avg       0.79      0.79      0.79       214\n",
      "\n",
      "model 2 accuracy using the validation data is: 80.37%\n",
      "\n",
      "confusion matrix for model 2 using the validation data: \n",
      "[[118  14]\n",
      " [ 28  54]]\n",
      "\n",
      "classification report for model 2 using the validation data:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85       132\n",
      "           1       0.79      0.66      0.72        82\n",
      "\n",
      "    accuracy                           0.80       214\n",
      "   macro avg       0.80      0.78      0.78       214\n",
      "weighted avg       0.80      0.80      0.80       214\n",
      "\n",
      "model 3 accuracy using the validation data is: 79.91%\n",
      "\n",
      "confusion matrix for model 3 using the validation data: \n",
      "[[116  16]\n",
      " [ 27  55]]\n",
      "\n",
      "classification report for model 3 using the validation data:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       132\n",
      "           1       0.77      0.67      0.72        82\n",
      "\n",
      "    accuracy                           0.80       214\n",
      "   macro avg       0.79      0.77      0.78       214\n",
      "weighted avg       0.80      0.80      0.80       214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 1 score\n",
    "print(f'model 1 accuracy using the validation data is: {rf1.score(X_val, y_val):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 1\n",
    "print(f'confusion matrix for model 1 using the validation data: \\n{confusion_matrix(y_val, val_1_preds)}\\n')\n",
    "\n",
    "# classification report for model 1\n",
    "print(f'classification report for model 1 using the validation data:\\n\\n \\\n",
    "{classification_report(y_val, val_1_preds)}')\n",
    "\n",
    "# model 2 score\n",
    "print(f'model 2 accuracy using the validation data is: {rf2.score(X_val, y_val):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 2\n",
    "print(f'confusion matrix for model 2 using the validation data: \\n{confusion_matrix(y_val, val_2_preds)}\\n')\n",
    "\n",
    "# classification report for model 2\n",
    "print(f'classification report for model 2 using the validation data:\\n\\n \\\n",
    "{classification_report(y_val, val_2_preds)}')\n",
    "\n",
    "# model 3 score\n",
    "print(f'model 3 accuracy using the validation data is: {rf3.score(X_val, y_val):.2%}\\n')\n",
    "\n",
    "# confusion matrix for model 3\n",
    "print(f'confusion matrix for model 3 using the validation data: \\n{confusion_matrix(y_val, val_3_preds)}\\n')\n",
    "\n",
    "# classification report for model 3\n",
    "print(f'classification report for model 3 using the validation data:\\n\\n \\\n",
    "{classification_report(y_val, val_3_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6f20159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.97999999999999"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "87.35 - 80.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82fb1074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.439999999999998"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "86.35 - 79.91"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37eb87",
   "metadata": {},
   "source": [
    "Model 3 with max_depth=5, min_samples_leaf=5 had the lowest drop in accuracy from the training data and the validation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
